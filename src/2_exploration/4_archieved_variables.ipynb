{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import scipy.cluster.hierarchy as spc\n",
    "from sklearn.utils import resample\n",
    "\n",
    "pio.templates.default = \"plotly_white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d722f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _ = first_run\n",
    "except NameError:\n",
    "    first_run = True\n",
    "    os.chdir(os.getcwd().rsplit(\"/\", 1)[0])\n",
    "    from _aux import functions as func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4553698",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "default = (\n",
    "    pd.read_csv(\n",
    "        \"../data/train/X_train.csv\",\n",
    "        index_col=0,\n",
    "        usecols=[\n",
    "            \"row_id\",\n",
    "            \"num_arch_dc_0_12m\",\n",
    "            \"num_arch_dc_12_24m\",\n",
    "            \"num_arch_ok_0_12m\",\n",
    "            \"num_arch_ok_12_24m\",\n",
    "            \"num_arch_rem_0_12m\",\n",
    "            \"num_arch_written_off_0_12m\",\n",
    "            \"num_arch_written_off_12_24m\",\n",
    "        ],\n",
    "    )\n",
    "    .join(pd.read_csv(\"../data/train/y_train.csv\", index_col=0))\n",
    "    .query(\"default == 1\")\n",
    ")\n",
    "\n",
    "not_default = (\n",
    "    pd.read_csv(\n",
    "        \"../data/train/X_train.csv\",\n",
    "        index_col=0,\n",
    "        usecols=[\n",
    "            \"row_id\",\n",
    "            \"num_arch_dc_0_12m\",\n",
    "            \"num_arch_dc_12_24m\",\n",
    "            \"num_arch_ok_0_12m\",\n",
    "            \"num_arch_ok_12_24m\",\n",
    "            \"num_arch_rem_0_12m\",\n",
    "            \"num_arch_written_off_0_12m\",\n",
    "            \"num_arch_written_off_12_24m\",\n",
    "        ],\n",
    "    )\n",
    "    .join(pd.read_csv(\"../data/train/y_train.csv\", index_col=0))\n",
    "    .query(\"default == 0\")\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"../data/train/X_train.csv\",\n",
    "    index_col=0,\n",
    ").join(pd.read_csv(\"../data/train/y_train.csv\", index_col=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d991f2",
   "metadata": {},
   "source": [
    "## The problematic variable\n",
    "\n",
    "Inspite of it seeming to be a good idea to keep track of invoices that have not being paid and become loss, both variables that strive for this function are utterly problematic. Both \"num_arch_written_off_0_12m\" and \"num_arch_written_off_12_24m\" have 18% of missing values, which in itself is not a problem. The issue here lies in the fact that less than 0.1% of observations are non-zero, which means that it has an extremely low signal-to-noise ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"num_arch_written_off_0_12m\", \"num_arch_written_off_12_24m\"]].replace(\n",
    "    0, np.nan\n",
    ").info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becb7929",
   "metadata": {},
   "source": [
    "But perhaps, those cases in which they are non-zero must be an extremely good predictor of default, right? Wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad333973",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        (\n",
    "            df.query(\"num_arch_written_off_0_12m > 0\")\n",
    "            .agg(\n",
    "                default=(\"default\", \"sum\"),\n",
    "                not_default=(\"default\", func.complement),\n",
    "                count=(\"default\", \"count\"),\n",
    "            )\n",
    "            .squeeze()\n",
    "            .to_frame(name=\"num_arch_written_off_0_12m\")\n",
    "        ),\n",
    "        (\n",
    "            df.query(\"num_arch_written_off_12_24m > 0\")\n",
    "            .agg(\n",
    "                default=(\"default\", \"sum\"),\n",
    "                not_default=(\"default\", func.complement),\n",
    "                count=(\"default\", \"count\"),\n",
    "            )\n",
    "            .squeeze()\n",
    "            .to_frame(name=\"num_arch_written_off_12_24m\")\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33fb0e",
   "metadata": {},
   "source": [
    "Therefore, we will drop these variables from our exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3152c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "default = default.drop(\n",
    "    [\"num_arch_written_off_0_12m\", \"num_arch_written_off_12_24m\"], axis=1\n",
    ")\n",
    "not_default = not_default.drop(\n",
    "    [\"num_arch_written_off_0_12m\", \"num_arch_written_off_12_24m\"], axis=1\n",
    ")\n",
    "df = df.drop([\"num_arch_written_off_0_12m\", \"num_arch_written_off_12_24m\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee24dd0",
   "metadata": {},
   "source": [
    "## Correlation between \"archived\" variables\n",
    "\n",
    "As shown in our sanity profile report, \"archieved\" variables have high correlation amongst themselves mainly due to overlaping lookback windows for aggregation. The first thing we must do is choose one (or some) of them to represent the group, which can be achieved by correlation clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebdcca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = pd.concat([default, not_default]).corr()\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=corr.mask(mask),\n",
    "        x=corr.columns,\n",
    "        y=corr.columns,\n",
    "        colorscale=px.colors.diverging.RdBu,\n",
    "        zmin=-1,\n",
    "        zmax=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Correlation between 'account' variables\",\n",
    "    yaxis_autorange=\"reversed\",\n",
    "    xaxis_showgrid=False,\n",
    "    yaxis_showgrid=False,\n",
    "    width=1000,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "# fig.update_traces(opacity=0.6)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed4680",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdist = spc.distance.pdist(corr)\n",
    "linkage = spc.linkage(pdist, method=\"single\")\n",
    "idx = spc.fcluster(linkage, 0.6 * pdist.max(), \"distance\")\n",
    "\n",
    "columns = [default.columns.tolist()[i] for i in list((np.argsort(idx)))]\n",
    "clusterd_corr = pd.concat([default, not_default]).reindex(columns, axis=1).corr()\n",
    "\n",
    "mask = np.zeros_like(clusterd_corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "corr[\"default\"].to_frame(name=\"corr_with_label\").assign(cluster=idx).drop(\n",
    "    \"default\"\n",
    ").sort_values([\"cluster\", \"corr_with_label\"], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a31ec1",
   "metadata": {},
   "source": [
    "Let's get an overview on our clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca392a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        df[[\"num_arch_ok_0_12m\", \"num_arch_ok_12_24m\"]],\n",
    "        df[[\"num_arch_dc_0_12m\", \"num_arch_dc_12_24m\", \"num_arch_rem_0_12m\"]],\n",
    "    ],\n",
    "    keys=[\"cluster_1\", \"cluster_2\"],\n",
    "    axis=1,\n",
    ").describe(np.append(np.arange(0.25, 1.0, 0.1), np.array([0.99])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f188f076",
   "metadata": {},
   "source": [
    "## 1. \"num_arch_dc_0_12m\"\n",
    "\n",
    "Despite having higher correlation with the target label, \"cluster 2\" variables have mostly zero values. Let's check how defaults behave across values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "func.test_k_prop(\n",
    "    df.groupby(\"num_arch_dc_0_12m\").agg(\n",
    "        default=(\"default\", \"sum\"),\n",
    "        not_default=(\"default\", func.complement),\n",
    "        count=(\"default\", \"count\"),\n",
    "        contamination=(\"default\", lambda s: s.sum() / s.shape[0]),\n",
    "    )\n",
    ")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba9e06",
   "metadata": {},
   "source": [
    "We can observe that contamination rates increase, with some exceptions, for higher number of archived invoices of status \"dc\". Could we gain any benefit from transforming it into a boolean variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa73e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.assign(var_bool=df[\"num_arch_dc_0_12m\"] > 1)\n",
    "    .groupby(\"var_bool\")\n",
    "    .agg(\n",
    "        default=(\"default\", \"sum\"),\n",
    "        not_default=(\"default\", func.complement),\n",
    "        count=(\"default\", \"count\"),\n",
    "        contamination=(\"default\", lambda s: s.sum() / s.shape[0]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edb20b2",
   "metadata": {},
   "source": [
    "Once again we see that transforming the variable into boolean does provide a subclass with much higher contamination rate.\n",
    "We conclude that, despite having a high concentrarion of zero values, this variable (and its boolean counterpart) could be useful as features for our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f078474",
   "metadata": {},
   "source": [
    "## 2. \"num_arch_ok_0_12m\"\n",
    "\n",
    "This variable has too many unique values to be considered categorical. We can take 3 different approaches to explore it: numerical, boolean and binned.\n",
    "\n",
    "Let's first look at it as a numerical variable and verify whether its behavious for default and not_default differs significantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=not_default.num_arch_ok_0_12m.sample(1000, replace=True, random_state=42),\n",
    "        name=\"not_default\",\n",
    "        histfunc=\"count\",\n",
    "        # histnorm='probability',\n",
    "        xbins=dict(start=-1, end=50, size=1),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=default.num_arch_ok_0_12m.sample(1000, replace=True, random_state=42),\n",
    "        name=\"default\",\n",
    "        histfunc=\"count\",\n",
    "        # histnorm='probability',\n",
    "        xbins=dict(start=-1, end=50, size=1),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(title=\"Title\", barmode=\"overlay\")\n",
    "\n",
    "fig.update_traces(opacity=0.6)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08469fea",
   "metadata": {},
   "source": [
    "Apparently, customers who default tend to have smaller values for \"num_arch_ok_0_12m\", which is in line with common sense. Users who have made successfull purchases within the last 12 months should be expected to be more reliable payers. Of course, one could lose their job, fall ill and be a victim of financial crime, all of which would impair their ability to pay their debt. Thus, this variable is not expected to perfectly explain the target label, although it is a good candidate for feature.\n",
    "\n",
    "Let's examine whether the difference between \"default\" and \"not_default\" is indeed significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21738cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 100_000\n",
    "sample1 = []\n",
    "sample2 = []\n",
    "combined = np.concatenate(\n",
    "    (default.num_arch_ok_0_12m, not_default.num_arch_ok_0_12m), axis=0\n",
    ")\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    np.random.seed(i)\n",
    "    combined = np.concatenate(\n",
    "        (\n",
    "            default.num_arch_ok_0_12m.sample(1_000, replace=True),\n",
    "            not_default.num_arch_ok_0_12m.sample(1_000, replace=True),\n",
    "        ),\n",
    "        axis=0,\n",
    "    )\n",
    "    sample1.append(resample(combined, n_samples=500))\n",
    "    sample2.append(resample(combined, n_samples=500))\n",
    "\n",
    "diff_bootstrap_medians = np.median(sample1, axis=1) - np.median(sample2, axis=1)\n",
    "\n",
    "observed_difference = np.median(\n",
    "    default.num_arch_ok_0_12m.sample(1_000, replace=True, random_state=42)\n",
    ") - np.median(\n",
    "    not_default.num_arch_ok_0_12m.sample(1_000, replace=True, random_state=42)\n",
    ")\n",
    "\n",
    "p_value = (\n",
    "    diff_bootstrap_medians[diff_bootstrap_medians < observed_difference].shape[0]\n",
    "    / num_iterations\n",
    ")\n",
    "\n",
    "ci_lower, ci_upper = np.percentile(diff_bootstrap_medians, [0.5, 99.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b02af95",
   "metadata": {},
   "source": [
    "Due to the excessively right skewed and platykurtic distribution of \"not_default\" for this variable, we choose to compare medians instead of means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df96d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=diff_bootstrap_means,\n",
    "        name=\"sample_difference\",\n",
    "        histfunc=\"count\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=observed_difference,\n",
    "    line_width=3,\n",
    "    line_color=\"indianred\",\n",
    "    line_dash=\"dash\",\n",
    "    annotation_text=f\"Observed\",\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=ci_lower,\n",
    "    line_width=3,\n",
    "    line_color=\"indianred\",\n",
    "    line_dash=\"solid\",\n",
    "    annotation_text=f\"0.5%\",\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=ci_upper,\n",
    "    line_width=3,\n",
    "    line_color=\"indianred\",\n",
    "    line_dash=\"solid\",\n",
    "    annotation_text=f\"99.5%\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Distribution of difference in medians\",\n",
    "    barmode=\"overlay\",\n",
    ")\n",
    "\n",
    "fig.update_traces(opacity=0.75)\n",
    "# fig.update_xaxes(range=[-5, 5])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad014db0",
   "metadata": {},
   "source": [
    "This result shows us that there is a significative difference in medians, which in turn suggests that this variable could be useful for model induction. Now, we analyze whether its binned counterpart is too a good candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8f9251",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df[[\"default\"]]\n",
    "    .assign(\n",
    "        bins_var=pd.cut(df[\"num_arch_ok_0_12m\"], 10),\n",
    "    )\n",
    "    .groupby(\"bins_var\")\n",
    "    .agg(\n",
    "        default=(\"default\", \"sum\"),\n",
    "        not_default=(\"default\", func.complement),\n",
    "        count=(\"default\", \"count\"),\n",
    "        contamination=(\"default\", lambda s: s.sum() / s.shape[0]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9851f5",
   "metadata": {},
   "source": [
    "The fact that using 10 bins has very little effect on the contamination rate shows that this is not a good strategy to move forward with. Hopefully, its boolean counterpart can perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a9e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df[[\"default\"]]\n",
    "    .assign(\n",
    "        bool_var=df[\"num_arch_ok_0_12m\"] > 1,\n",
    "    )\n",
    "    .groupby(\"bool_var\")\n",
    "    .agg(\n",
    "        default=(\"default\", \"sum\"),\n",
    "        not_default=(\"default\", func.complement),\n",
    "        count=(\"default\", \"count\"),\n",
    "        contamination=(\"default\", lambda s: s.sum() / s.shape[0]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945fc81d",
   "metadata": {},
   "source": [
    "The boolean variable does increase contamination in 2 fold for the group \"up to 1\" at the expense of a very diluted rate for the \"above 1\" group, which wouldn't be a problem if the latter held few \"default\" observations. However, the group holds almost 20% of \"default\" observations, thus making the boolean variable also unsuitable.\n",
    "\n",
    "--\n",
    "\n",
    "To sum up, we show that \"num_arch_dc_0_12m\" and its boolean counterpart are good candidates for features.\n",
    "Furthermore, we test three variations of \"num_arch_ok_0_12m\", which showed to be best in its original form.\n",
    "Hence, we move forward with 3 candidates for features from \"archieved\" variables:\n",
    "- num_arch_dc_0_12m\n",
    "- num_arch_dc_0_12m_is_above_1\n",
    "- num_arch_ok_0_12m\n",
    "\n",
    "Next, we look at \"order\" variables."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('klarna': conda)",
   "name": "python388jvsc74a57bd09f119b1d3a8a63730ae7ee508102d23a56b995c1f2248036bd772c0398b7d40e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}