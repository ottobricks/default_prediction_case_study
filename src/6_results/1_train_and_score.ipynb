{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0643d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Import all candidates to support automatic decision making\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874cf3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _ = first_run\n",
    "except NameError:\n",
    "    first_run = True\n",
    "    os.chdir(os.getcwd().rsplit(\"/\", 1)[0])\n",
    "    from _aux import features as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c28e2",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Now that we are ready to present our final results, we must load the test set which we held out at step \"0_split_data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = joblib.load(\"../data/train/preprocessed/undersampled_train_features_labels.joblib.gz\")\n",
    "\n",
    "X_test, y_test = pd.read_csv(\"../data/test/X_test.csv\", index_col=0), pd.read_csv(\"../data/test/y_test.csv\", index_col=0)"
   ]
  },
  {
   "source": [
    "# Load the preprocessor and transform test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = joblib.load(\"../ml_artifacts/preprocessor.joblib.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preproc = preprocessor.transform(X_test)"
   ]
  },
  {
   "source": [
    "# Choose model based on GridSearch performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = joblib.load(\"../ml_artifacts/gridsearch_results/param_search_result.joblib.gz\").iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3d80d",
   "metadata": {},
   "source": [
    "# Fit the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = eval(best_model.estimator)(**best_model.params).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict_proba(X_test_preproc)\n",
    "\n",
    "threshold_perf = pd.DataFrame(\n",
    "    [\n",
    "        (threshold, *confusion_matrix(y_test, (prediction[:, 1] > threshold).astype(int)).ravel())\n",
    "        for threshold in np.arange(.05, 1, .05)\n",
    "    ],\n",
    "    columns=[\"threshold\", \"tn\", \"fp\", \"fn\", \"tp\"]\n",
    ").assign(\n",
    "    precision=lambda df: df[\"tp\"] / (df[\"tp\"] + df[\"fp\"]),\n",
    "    recall=lambda df: df[\"tp\"] / (df[\"tp\"] + df[\"fn\"]),\n",
    "    f1=lambda df: 2 * (df[\"precision\"] * df[\"recall\"]) / (df[\"precision\"] + df[\"recall\"])\n",
    ")\n",
    "\n",
    "threshold_perf.to_csv(\"../ml_artifacts/model_performance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(data, color='yellow'):\n",
    "    '''\n",
    "    highlight the maximum in a Series or DataFrame\n",
    "    '''\n",
    "    attr = 'background-color: {}'.format(color)\n",
    "    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n",
    "        is_max = data == data.max()\n",
    "        return [attr if v else '' for v in is_max]\n",
    "    else:  # from .apply(axis=None)\n",
    "        is_max = data == data.max().max()\n",
    "        return pd.DataFrame(np.where(is_max, attr, ''),\n",
    "                            index=data.index, columns=data.columns)\n",
    "\n",
    "\n",
    "threshold_perf.style.apply(\n",
    "    highlight_max, color='green', subset=[\"precision\", \"recall\", 'f1']\n",
    ")"
   ]
  },
  {
   "source": [
    "# Add model to pipeline and save it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = preprocessor.set_params(model=model)\n",
    "\n",
    "joblib.dump(pipeline, \"../ml_artifacts/pipeline.joblib.gz\")"
   ]
  },
  {
   "source": [
    "# Batch predict required set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = pd.read_csv(\"../data/predict/to_predict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict[[\"uuid\"]].assign(\n",
    "    pd=pipeline.predict(to_predict.drop(\"default\", axis=1))\n",
    ").to_csv(\"../data/predict/predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "name": "python38864bitklarnaconda4ef8c56e5567458cb1bf905cc9704f70",
   "display_name": "Python 3.8.8 64-bit ('klarna': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}