{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0643d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "874cf3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _ = first_run\n",
    "except NameError:\n",
    "    first_run = True\n",
    "    os.chdir(os.getcwd().rsplit(\"/\", 1)[0])\n",
    "    from _aux import ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c28e2",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d99414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = joblib.load(\"../data/train/preprocessed/train_features_labels.joblib.gz\")\n",
    "\n",
    "X_validation, y_validation = joblib.load(\"../data/train/preprocessed/validation_features_labels.joblib.gz\")"
   ]
  },
  {
   "source": [
    "# Hold your SMOTE for a moment\n",
    "\n",
    "SMOTE has become a ubiquitous way to handle imbalanced classes by oversampling the minority class. However, the fact that many of our features have low variance due to a lot of zero values, generating artificial samples from them can actually become quite counterproductive. Thus, we will experiment with both SMOTE and a custom undersampler that tries to capture most of the variance of the majority class. Whichever strategy yields better results for our baseline model will be the one we move forward with."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1. Custom undersampler"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_maj, y_train_maj, sample_variance, sample_variance_zscore, is_significant = ml.BinaryUndersampler(n_iterations=1_000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1627, 18)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Create new training set\n",
    "X_train_undersample = np.concatenate((X_train_maj, X_train[y_train==1]))\n",
    "y_train_undersample = np.concatenate((y_train_maj, y_train[y_train==1]))\n",
    "\n",
    "# Save the new trainig set\n",
    "joblib.dump([], \"\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "name": "python388jvsc74a57bd09f119b1d3a8a63730ae7ee508102d23a56b995c1f2248036bd772c0398b7d40e",
   "display_name": "Python 3.8.8 64-bit ('klarna': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}